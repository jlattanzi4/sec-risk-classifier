{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Phase 2: Text Preprocessing & Risk Segmentation\n",
    "\n",
    "## SEC 10-K Risk Factor Intelligence\n",
    "\n",
    "This notebook covers:\n",
    "1. Text cleaning (HTML, navigation text, normalization)\n",
    "2. Sentence and paragraph segmentation\n",
    "3. Risk paragraph extraction\n",
    "4. Initial risk category identification\n",
    "5. Building reusable preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load spaCy model for sentence segmentation\n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "except OSError:\n",
    "    print('Downloading spaCy model...')\n",
    "    !python -m spacy download en_core_web_sm\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Increase max length for long documents\n",
    "nlp.max_length = 600000\n",
    "\n",
    "print('Libraries loaded successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Phase 1 dataset\n",
    "df = pd.read_parquet('../data/processed/risk_factors_2006_2020.parquet')\n",
    "print(f'Loaded {len(df):,} filings')\n",
    "print(f'Columns: {df.columns.tolist()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 1. Text Cleaning Pipeline\n",
    "\n",
    "Build functions to clean and normalize the risk factor text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_risk_text(text):\n",
    "    \"\"\"\n",
    "    Clean and normalize risk factor text.\n",
    "    \n",
    "    Steps:\n",
    "    1. Remove HTML tags (rare but present)\n",
    "    2. Remove navigation text (Back to Table of Contents)\n",
    "    3. Remove Item 1A header\n",
    "    4. Normalize bullet characters\n",
    "    5. Normalize whitespace\n",
    "    6. Remove page numbers/headers\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    \n",
    "    # 1. Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)\n",
    "    \n",
    "    # 2. Remove HTML entities\n",
    "    text = re.sub(r'&nbsp;|&#160;|&amp;|&quot;|&lt;|&gt;', ' ', text)\n",
    "    \n",
    "    # 3. Remove navigation text\n",
    "    text = re.sub(r'back to (table of )?contents', '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # 4. Remove the Item 1A header itself (we know what section this is)\n",
    "    text = re.sub(r'^\\s*item\\s*1a\\.?\\s*:?\\s*risk\\s*factors\\s*', '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # 5. Normalize bullet characters to standard bullet\n",
    "    text = re.sub(r'[•●○◦▪▫◘►▸‣⁃]', '•', text)\n",
    "    \n",
    "    # 6. Normalize whitespace (but preserve paragraph breaks)\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)  # Multiple spaces to single\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)  # Max 2 newlines\n",
    "    \n",
    "    # 7. Remove standalone page numbers\n",
    "    text = re.sub(r'^\\s*\\d+\\s*$', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # 8. Remove common header/footer patterns\n",
    "    text = re.sub(r'\\b\\d+\\s*of\\s*\\d+\\b', '', text)  # \"Page X of Y\"\n",
    "    text = re.sub(r'table of contents', '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Test on a sample\n",
    "sample_text = df['section_1A'].iloc[0]\n",
    "cleaned = clean_risk_text(sample_text)\n",
    "print(f'Original length: {len(sample_text):,}')\n",
    "print(f'Cleaned length: {len(cleaned):,}')\n",
    "print(f'\\nFirst 500 chars of cleaned text:')\n",
    "print(cleaned[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply cleaning to all documents\n",
    "print('Cleaning all documents...')\n",
    "tqdm.pandas(desc='Cleaning')\n",
    "df['clean_text'] = df['section_1A'].progress_apply(clean_risk_text)\n",
    "\n",
    "# Calculate new lengths\n",
    "df['clean_length'] = df['clean_text'].str.len()\n",
    "\n",
    "print(f'\\nCleaning complete!')\n",
    "print(f'Average reduction: {(1 - df[\"clean_length\"].mean() / df[\"item_1a_length\"].mean()) * 100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 2. Risk Paragraph Segmentation\n",
    "\n",
    "SEC filings typically organize risks as:\n",
    "- **Risk Header** (bold or capitalized title)\n",
    "- **Risk Description** (1-3 paragraphs explaining the risk)\n",
    "\n",
    "We'll segment by identifying risk headers and their associated content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_risk_paragraphs(text):\n",
    "    \"\"\"\n",
    "    Extract individual risk paragraphs from Item 1A text.\n",
    "    \n",
    "    Strategy:\n",
    "    1. Look for risk headers (sentences ending with period, followed by paragraph)\n",
    "    2. Common patterns: \"We may...\", \"Our business could...\", \"Risk of...\"\n",
    "    3. Fall back to paragraph-based segmentation\n",
    "    \n",
    "    Returns:\n",
    "        list: List of (header, content) tuples\n",
    "    \"\"\"\n",
    "    if not text or len(text) < 100:\n",
    "        return []\n",
    "    \n",
    "    risks = []\n",
    "    \n",
    "    # Pattern 1: Look for risk headers (short sentences that introduce risks)\n",
    "    # Common formats:\n",
    "    # - \"We depend on key personnel.\" followed by explanation\n",
    "    # - \"Competition\" or \"Competition.\" as a header\n",
    "    # - Lines that are all caps or title case and short\n",
    "    \n",
    "    # Split into paragraphs first\n",
    "    paragraphs = re.split(r'\\n\\s*\\n', text)\n",
    "    paragraphs = [p.strip() for p in paragraphs if p.strip()]\n",
    "    \n",
    "    current_header = None\n",
    "    current_content = []\n",
    "    \n",
    "    for para in paragraphs:\n",
    "        # Check if this looks like a header\n",
    "        is_header = False\n",
    "        \n",
    "        # Short paragraph (< 200 chars) that could be a header\n",
    "        if len(para) < 200:\n",
    "            # All caps\n",
    "            if para.isupper():\n",
    "                is_header = True\n",
    "            # Ends with period and is short (risk title)\n",
    "            elif para.endswith('.') and len(para) < 150:\n",
    "                # Check if it starts with risk-related words\n",
    "                if re.match(r'^(we |our |the |if |there |risks? |loss |failure |changes? |inability )', para, re.I):\n",
    "                    is_header = True\n",
    "            # Title case short line\n",
    "            elif para.istitle() and len(para) < 100:\n",
    "                is_header = True\n",
    "        \n",
    "        if is_header:\n",
    "            # Save previous risk if exists\n",
    "            if current_header and current_content:\n",
    "                risks.append({\n",
    "                    'header': current_header,\n",
    "                    'content': ' '.join(current_content)\n",
    "                })\n",
    "            current_header = para\n",
    "            current_content = []\n",
    "        else:\n",
    "            current_content.append(para)\n",
    "    \n",
    "    # Don't forget the last risk\n",
    "    if current_header and current_content:\n",
    "        risks.append({\n",
    "            'header': current_header,\n",
    "            'content': ' '.join(current_content)\n",
    "        })\n",
    "    \n",
    "    # If no headers found, fall back to paragraph-based segmentation\n",
    "    if not risks and paragraphs:\n",
    "        for i, para in enumerate(paragraphs):\n",
    "            if len(para) > 100:  # Only substantial paragraphs\n",
    "                risks.append({\n",
    "                    'header': f'Risk {i+1}',\n",
    "                    'content': para\n",
    "                })\n",
    "    \n",
    "    return risks\n",
    "\n",
    "# Test on a sample\n",
    "sample_risks = extract_risk_paragraphs(df['clean_text'].iloc[100])\n",
    "print(f'Found {len(sample_risks)} risk paragraphs')\n",
    "print('\\nFirst 3 risks:')\n",
    "for i, risk in enumerate(sample_risks[:3]):\n",
    "    print(f'\\n--- Risk {i+1} ---')\n",
    "    print(f'Header: {risk[\"header\"][:100]}...' if len(risk['header']) > 100 else f'Header: {risk[\"header\"]}')\n",
    "    print(f'Content: {risk[\"content\"][:200]}...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract risk paragraphs from all documents\n",
    "print('Extracting risk paragraphs from all documents...')\n",
    "tqdm.pandas(desc='Extracting')\n",
    "df['risk_paragraphs'] = df['clean_text'].progress_apply(extract_risk_paragraphs)\n",
    "df['num_risks'] = df['risk_paragraphs'].apply(len)\n",
    "\n",
    "print(f'\\nExtraction complete!')\n",
    "print(f'Total risk paragraphs extracted: {df[\"num_risks\"].sum():,}')\n",
    "print(f'Average risks per filing: {df[\"num_risks\"].mean():.1f}')\n",
    "print(f'Median risks per filing: {df[\"num_risks\"].median():.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of risks per filing\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Histogram of risks per filing\n",
    "axes[0].hist(df['num_risks'], bins=50, color='steelblue', edgecolor='white')\n",
    "axes[0].axvline(df['num_risks'].median(), color='red', linestyle='--', label=f'Median: {df[\"num_risks\"].median():.0f}')\n",
    "axes[0].set_xlabel('Number of Risk Paragraphs')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Distribution of Risk Paragraphs per Filing')\n",
    "axes[0].legend()\n",
    "\n",
    "# Average risks by year\n",
    "risks_by_year = df.groupby('filing_year')['num_risks'].mean()\n",
    "axes[1].plot(risks_by_year.index, risks_by_year.values, marker='o', color='steelblue', linewidth=2)\n",
    "axes[1].set_xlabel('Year')\n",
    "axes[1].set_ylabel('Average Risk Paragraphs')\n",
    "axes[1].set_title('Average Risk Paragraphs Over Time')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/risk_segmentation.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 3. Sentence-Level Segmentation\n",
    "\n",
    "For fine-grained classification, we also need sentence-level segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences(text, max_sentences=500):\n",
    "    \"\"\"\n",
    "    Extract sentences from text using spaCy.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text\n",
    "        max_sentences: Maximum sentences to extract (for very long docs)\n",
    "    \n",
    "    Returns:\n",
    "        list: List of sentence strings\n",
    "    \"\"\"\n",
    "    if not text or len(text) < 50:\n",
    "        return []\n",
    "    \n",
    "    # Process with spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    sentences = []\n",
    "    for sent in doc.sents:\n",
    "        sent_text = sent.text.strip()\n",
    "        # Filter out very short or very long sentences\n",
    "        if 20 < len(sent_text) < 2000:\n",
    "            sentences.append(sent_text)\n",
    "        if len(sentences) >= max_sentences:\n",
    "            break\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "# Test on a sample\n",
    "sample_sentences = extract_sentences(df['clean_text'].iloc[0])\n",
    "print(f'Found {len(sample_sentences)} sentences')\n",
    "print('\\nFirst 5 sentences:')\n",
    "for i, sent in enumerate(sample_sentences[:5]):\n",
    "    print(f'{i+1}. {sent[:100]}...' if len(sent) > 100 else f'{i+1}. {sent}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 4. Initial Risk Category Identification\n",
    "\n",
    "Based on the project plan, we'll identify these risk categories:\n",
    "1. Regulatory/Legal\n",
    "2. Cybersecurity/Data Privacy\n",
    "3. Competitive/Market\n",
    "4. Macroeconomic\n",
    "5. Operational/Supply Chain\n",
    "6. Financial/Liquidity\n",
    "7. Environmental/Climate\n",
    "8. Personnel/Labor\n",
    "9. Reputational\n",
    "10. Technology/Innovation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define keyword patterns for initial category identification\n",
    "RISK_CATEGORIES = {\n",
    "    'regulatory_legal': [\n",
    "        r'regulat', r'compliance', r'legal', r'litigation', r'lawsuit',\n",
    "        r'government', r'legislation', r'law ', r'court', r'patent',\n",
    "        r'intellectual property', r'SEC', r'FDA', r'EPA', r'FTC'\n",
    "    ],\n",
    "    'cybersecurity': [\n",
    "        r'cyber', r'data breach', r'security breach', r'hack', r'privacy',\n",
    "        r'personal data', r'data protection', r'information security',\n",
    "        r'GDPR', r'CCPA', r'ransomware', r'malware'\n",
    "    ],\n",
    "    'competitive_market': [\n",
    "        r'compet', r'market share', r'pricing pressure', r'new entrants',\n",
    "        r'industry consolidation', r'customer concentration', r'demand'\n",
    "    ],\n",
    "    'macroeconomic': [\n",
    "        r'economic', r'recession', r'inflation', r'interest rate',\n",
    "        r'currency', r'exchange rate', r'GDP', r'unemployment',\n",
    "        r'global economy', r'trade war', r'tariff'\n",
    "    ],\n",
    "    'operational_supply': [\n",
    "        r'supply chain', r'supplier', r'manufacturing', r'production',\n",
    "        r'distribution', r'logistics', r'inventory', r'sourcing',\n",
    "        r'operational', r'disruption'\n",
    "    ],\n",
    "    'financial_liquidity': [\n",
    "        r'liquidity', r'cash flow', r'debt', r'credit', r'financing',\n",
    "        r'capital', r'covenant', r'leverage', r'bankruptcy', r'insolven'\n",
    "    ],\n",
    "    'environmental_climate': [\n",
    "        r'environment', r'climate', r'emission', r'carbon', r'pollution',\n",
    "        r'sustainability', r'renewable', r'ESG', r'natural disaster',\n",
    "        r'weather', r'flood', r'hurricane'\n",
    "    ],\n",
    "    'personnel_labor': [\n",
    "        r'employee', r'personnel', r'labor', r'workforce', r'talent',\n",
    "        r'key person', r'executive', r'management team', r'union',\n",
    "        r'hiring', r'retention'\n",
    "    ],\n",
    "    'reputational': [\n",
    "        r'reputation', r'brand', r'public perception', r'media',\n",
    "        r'social media', r'negative publicity', r'trust'\n",
    "    ],\n",
    "    'technology_innovation': [\n",
    "        r'technology', r'innovation', r'obsolete', r'R&D', r'research',\n",
    "        r'product development', r'digital', r'AI ', r'artificial intelligence',\n",
    "        r'automation', r'disrupt'\n",
    "    ]\n",
    "}\n",
    "\n",
    "def classify_risk_paragraph(text):\n",
    "    \"\"\"\n",
    "    Classify a risk paragraph into categories using keyword matching.\n",
    "    This is a preliminary classifier - will be improved with ML.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Category scores (0-1 based on keyword matches)\n",
    "    \"\"\"\n",
    "    text_lower = text.lower()\n",
    "    scores = {}\n",
    "    \n",
    "    for category, patterns in RISK_CATEGORIES.items():\n",
    "        matches = sum(1 for p in patterns if re.search(p, text_lower))\n",
    "        scores[category] = matches / len(patterns)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "def get_primary_category(scores):\n",
    "    \"\"\"Get the primary category from scores dict.\"\"\"\n",
    "    if not scores or max(scores.values()) == 0:\n",
    "        return 'other'\n",
    "    return max(scores, key=scores.get)\n",
    "\n",
    "# Test on sample risks\n",
    "sample_risk = df['risk_paragraphs'].iloc[100][0] if df['risk_paragraphs'].iloc[100] else None\n",
    "if sample_risk:\n",
    "    scores = classify_risk_paragraph(sample_risk['content'])\n",
    "    print(f'Risk: {sample_risk[\"header\"]}')\n",
    "    print(f'\\nCategory scores:')\n",
    "    for cat, score in sorted(scores.items(), key=lambda x: -x[1])[:5]:\n",
    "        print(f'  {cat}: {score:.2f}')\n",
    "    print(f'\\nPrimary category: {get_primary_category(scores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create flattened dataset of individual risk paragraphs\n",
    "print('Creating flattened risk paragraph dataset...')\n",
    "\n",
    "risk_records = []\n",
    "for _, row in tqdm(df.iterrows(), total=len(df), desc='Flattening'):\n",
    "    for risk in row['risk_paragraphs']:\n",
    "        scores = classify_risk_paragraph(risk['content'])\n",
    "        record = {\n",
    "            'cik': row['cik'],\n",
    "            'filing_year': row['filing_year'],\n",
    "            'filename': row['filename'],\n",
    "            'risk_header': risk['header'],\n",
    "            'risk_content': risk['content'],\n",
    "            'content_length': len(risk['content']),\n",
    "            'primary_category': get_primary_category(scores),\n",
    "            **{f'score_{k}': v for k, v in scores.items()}\n",
    "        }\n",
    "        risk_records.append(record)\n",
    "\n",
    "df_risks = pd.DataFrame(risk_records)\n",
    "print(f'\\nCreated dataset with {len(df_risks):,} individual risk paragraphs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of preliminary categories\n",
    "print('Preliminary category distribution:')\n",
    "category_counts = df_risks['primary_category'].value_counts()\n",
    "print(category_counts)\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "category_counts.plot(kind='bar', ax=ax, color='steelblue')\n",
    "ax.set_xlabel('Risk Category')\n",
    "ax.set_ylabel('Number of Risk Paragraphs')\n",
    "ax.set_title('Preliminary Risk Category Distribution (Keyword-based)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/preliminary_categories.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 5. Save Preprocessed Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save document-level dataset with clean text\n",
    "df_docs = df[['cik', 'filename', 'filing_year', 'clean_text', 'clean_length', 'num_risks']].copy()\n",
    "df_docs.to_parquet('../data/processed/risk_factors_cleaned.parquet', index=False)\n",
    "print(f'Saved document-level dataset: {len(df_docs):,} documents')\n",
    "\n",
    "# Save paragraph-level dataset\n",
    "df_risks.to_parquet('../data/processed/risk_paragraphs.parquet', index=False)\n",
    "print(f'Saved paragraph-level dataset: {len(df_risks):,} paragraphs')\n",
    "\n",
    "# Save a sample for quick inspection\n",
    "df_risks.sample(min(5000, len(df_risks))).to_csv('../data/processed/risk_paragraphs_sample.csv', index=False)\n",
    "print('Saved sample CSV for inspection')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 6. Create Reusable Preprocessing Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write preprocessing functions to a Python module\n",
    "preprocessing_code = '''\n",
    "\"\"\"Text preprocessing utilities for SEC Risk Factor classification.\"\"\"\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "# Load spaCy model\n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    nlp.max_length = 600000\n",
    "except OSError:\n",
    "    nlp = None\n",
    "\n",
    "\n",
    "def clean_risk_text(text):\n",
    "    \"\"\"Clean and normalize risk factor text.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)\n",
    "    \n",
    "    # Remove HTML entities\n",
    "    text = re.sub(r'&nbsp;|&#160;|&amp;|&quot;|&lt;|&gt;', ' ', text)\n",
    "    \n",
    "    # Remove navigation text\n",
    "    text = re.sub(r'back to (table of )?contents', '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Remove Item 1A header\n",
    "    text = re.sub(r'^\\\\s*item\\\\s*1a\\\\.?\\\\s*:?\\\\s*risk\\\\s*factors\\\\s*', '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Normalize bullet characters\n",
    "    text = re.sub(r'[•●○◦▪▫◘►▸‣⁃]', '•', text)\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'[ \\\\t]+', ' ', text)\n",
    "    text = re.sub(r'\\\\n{3,}', '\\\\n\\\\n', text)\n",
    "    \n",
    "    # Remove page numbers\n",
    "    text = re.sub(r'^\\\\s*\\\\d+\\\\s*$', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\\\b\\\\d+\\\\s*of\\\\s*\\\\d+\\\\b', '', text)\n",
    "    text = re.sub(r'table of contents', '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def extract_risk_paragraphs(text):\n",
    "    \"\"\"Extract individual risk paragraphs from Item 1A text.\"\"\"\n",
    "    if not text or len(text) < 100:\n",
    "        return []\n",
    "    \n",
    "    risks = []\n",
    "    paragraphs = re.split(r'\\\\n\\\\s*\\\\n', text)\n",
    "    paragraphs = [p.strip() for p in paragraphs if p.strip()]\n",
    "    \n",
    "    current_header = None\n",
    "    current_content = []\n",
    "    \n",
    "    for para in paragraphs:\n",
    "        is_header = False\n",
    "        \n",
    "        if len(para) < 200:\n",
    "            if para.isupper():\n",
    "                is_header = True\n",
    "            elif para.endswith('.') and len(para) < 150:\n",
    "                if re.match(r'^(we |our |the |if |there |risks? |loss |failure |changes? |inability )', para, re.I):\n",
    "                    is_header = True\n",
    "            elif para.istitle() and len(para) < 100:\n",
    "                is_header = True\n",
    "        \n",
    "        if is_header:\n",
    "            if current_header and current_content:\n",
    "                risks.append({\n",
    "                    'header': current_header,\n",
    "                    'content': ' '.join(current_content)\n",
    "                })\n",
    "            current_header = para\n",
    "            current_content = []\n",
    "        else:\n",
    "            current_content.append(para)\n",
    "    \n",
    "    if current_header and current_content:\n",
    "        risks.append({\n",
    "            'header': current_header,\n",
    "            'content': ' '.join(current_content)\n",
    "        })\n",
    "    \n",
    "    if not risks and paragraphs:\n",
    "        for i, para in enumerate(paragraphs):\n",
    "            if len(para) > 100:\n",
    "                risks.append({\n",
    "                    'header': f'Risk {i+1}',\n",
    "                    'content': para\n",
    "                })\n",
    "    \n",
    "    return risks\n",
    "\n",
    "\n",
    "def extract_sentences(text, max_sentences=500):\n",
    "    \"\"\"Extract sentences using spaCy.\"\"\"\n",
    "    if not text or len(text) < 50 or nlp is None:\n",
    "        return []\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    sentences = []\n",
    "    \n",
    "    for sent in doc.sents:\n",
    "        sent_text = sent.text.strip()\n",
    "        if 20 < len(sent_text) < 2000:\n",
    "            sentences.append(sent_text)\n",
    "        if len(sentences) >= max_sentences:\n",
    "            break\n",
    "    \n",
    "    return sentences\n",
    "'''\n",
    "\n",
    "with open('../src/preprocessing.py', 'w') as f:\n",
    "    f.write(preprocessing_code)\n",
    "\n",
    "print('Saved preprocessing module to src/preprocessing.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Phase 2 Summary\n",
    "\n",
    "### Completed\n",
    "- ✅ Text cleaning pipeline (HTML, navigation, normalization)\n",
    "- ✅ Risk paragraph segmentation\n",
    "- ✅ Preliminary category identification (keyword-based)\n",
    "- ✅ Created flattened paragraph-level dataset\n",
    "- ✅ Saved reusable preprocessing module\n",
    "\n",
    "### Key Statistics\n",
    "- Documents cleaned: [X]\n",
    "- Total risk paragraphs: [X]\n",
    "- Average risks per filing: [X]\n",
    "\n",
    "### Outputs\n",
    "- `data/processed/risk_factors_cleaned.parquet` - Document-level\n",
    "- `data/processed/risk_paragraphs.parquet` - Paragraph-level\n",
    "- `src/preprocessing.py` - Reusable module\n",
    "\n",
    "### Next Steps (Phase 3)\n",
    "- Manual labeling of ~500 risk paragraphs\n",
    "- Build TF-IDF baseline classifier\n",
    "- Establish evaluation metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
