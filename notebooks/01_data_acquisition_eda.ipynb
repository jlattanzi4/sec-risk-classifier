{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Data Acquisition & Exploration\n",
    "\n",
    "## SEC 10-K Risk Factor Intelligence\n",
    "\n",
    "This notebook covers:\n",
    "1. Loading EDGAR-CORPUS from Hugging Face\n",
    "2. Filtering to 10-K filings (2006-2020)\n",
    "3. Extracting Item 1A (Risk Factors) sections\n",
    "4. Manual sampling to understand document structure\n",
    "5. Basic EDA: document lengths, word counts, industry distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if needed\n",
    "# !pip install datasets pandas numpy matplotlib seaborn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print('Libraries loaded successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load EDGAR-CORPUS Dataset\n",
    "\n",
    "The EDGAR-CORPUS dataset on Hugging Face contains SEC filings from 1993-2020.\n",
    "We'll focus on 10-K filings from 2006-2020 (Item 1A required since 2005)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset - this may take a few minutes on first run\n",
    "# The dataset is organized by year\n",
    "print(\"Loading EDGAR-CORPUS dataset...\")\n",
    "print(\"Note: First load will download ~2GB of data\")\n",
    "\n",
    "# Load a single year first to understand the structure\n",
    "sample_year = load_dataset(\n",
    "    \"eloukas/edgar-corpus\",\n",
    "    \"year_2020\",\n",
    "    split=\"train\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset loaded for 2020\")\n",
    "print(f\"Number of filings: {len(sample_year)}\")\n",
    "print(f\"\\nColumns: {sample_year.column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine sample record structure\n",
    "sample_record = sample_year[0]\n",
    "print(\"Sample record keys:\")\n",
    "for key in sample_record.keys():\n",
    "    value = sample_record[key]\n",
    "    if isinstance(value, str) and len(value) > 100:\n",
    "        print(f\"  {key}: {type(value).__name__} (length: {len(value)})\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Multiple Years (2006-2020)\n",
    "\n",
    "Item 1A (Risk Factors) became mandatory in 2005, so we'll use 2006-2020 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all years from 2006-2020\n",
    "years = range(2006, 2021)\n",
    "all_filings = []\n",
    "\n",
    "for year in tqdm(years, desc=\"Loading years\"):\n",
    "    try:\n",
    "        year_data = load_dataset(\n",
    "            \"eloukas/edgar-corpus\",\n",
    "            f\"year_{year}\",\n",
    "            split=\"train\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        # Convert to pandas and add year column\n",
    "        df_year = year_data.to_pandas()\n",
    "        df_year['filing_year'] = year\n",
    "        all_filings.append(df_year)\n",
    "        print(f\"  {year}: {len(df_year)} filings\")\n",
    "    except Exception as e:\n",
    "        print(f\"  {year}: Error - {e}\")\n",
    "\n",
    "# Combine all years\n",
    "df_all = pd.concat(all_filings, ignore_index=True)\n",
    "print(f\"\\nTotal filings loaded: {len(df_all):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dataset structure\n",
    "print(\"Dataset Info:\")\n",
    "print(f\"Shape: {df_all.shape}\")\n",
    "print(f\"\\nColumns: {df_all.columns.tolist()}\")\n",
    "print(f\"\\nData types:\")\n",
    "print(df_all.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Filter to 10-K Filings Only\n",
    "\n",
    "The dataset contains multiple filing types. We need only 10-K (and 10-K/A amendments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check filing types if form_type column exists\n",
    "if 'form_type' in df_all.columns:\n",
    "    print(\"Filing types distribution:\")\n",
    "    print(df_all['form_type'].value_counts().head(20))\n",
    "elif 'filename' in df_all.columns:\n",
    "    # Try to extract form type from filename\n",
    "    print(\"Sample filenames:\")\n",
    "    print(df_all['filename'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to 10-K filings\n",
    "# Adjust this based on actual column names in the dataset\n",
    "if 'form_type' in df_all.columns:\n",
    "    df_10k = df_all[df_all['form_type'].str.contains('10-K', case=False, na=False)].copy()\n",
    "else:\n",
    "    # If form_type not available, we may need to infer from other fields\n",
    "    # The EDGAR-CORPUS dataset typically includes this in the filename or as metadata\n",
    "    df_10k = df_all.copy()  # Will filter in next step\n",
    "    print(\"Note: form_type column not found, proceeding with all data\")\n",
    "\n",
    "print(f\"\\n10-K filings: {len(df_10k):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extract Item 1A (Risk Factors) Section\n",
    "\n",
    "Item 1A contains the Risk Factors disclosure. We need to extract this section from each filing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_item_1a(text):\n",
    "    \"\"\"\n",
    "    Extract Item 1A (Risk Factors) section from 10-K filing text.\n",
    "    \n",
    "    Returns:\n",
    "        str: Extracted risk factors text, or None if not found\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or len(text) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Common patterns for Item 1A header\n",
    "    # Pattern to find start of Item 1A\n",
    "    start_patterns = [\n",
    "        r'item\\s*1a\\.?\\s*risk\\s*factors',\n",
    "        r'item\\s*1a\\.?\\s*-\\s*risk\\s*factors',\n",
    "        r'item\\s*1a\\s*[:\\.]\\s*risk\\s*factors',\n",
    "    ]\n",
    "    \n",
    "    # Pattern to find end (start of Item 1B or Item 2)\n",
    "    end_patterns = [\n",
    "        r'item\\s*1b\\.?\\s*unresolved\\s*staff\\s*comments',\n",
    "        r'item\\s*2\\.?\\s*properties',\n",
    "        r'item\\s*1b\\.',\n",
    "        r'item\\s*2\\.',\n",
    "    ]\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Find start position\n",
    "    start_pos = None\n",
    "    for pattern in start_patterns:\n",
    "        match = re.search(pattern, text_lower)\n",
    "        if match:\n",
    "            start_pos = match.start()\n",
    "            break\n",
    "    \n",
    "    if start_pos is None:\n",
    "        return None\n",
    "    \n",
    "    # Find end position\n",
    "    end_pos = len(text)\n",
    "    for pattern in end_patterns:\n",
    "        match = re.search(pattern, text_lower[start_pos + 100:])  # Search after start\n",
    "        if match:\n",
    "            candidate_end = start_pos + 100 + match.start()\n",
    "            if candidate_end < end_pos:\n",
    "                end_pos = candidate_end\n",
    "    \n",
    "    # Extract section\n",
    "    item_1a_text = text[start_pos:end_pos].strip()\n",
    "    \n",
    "    # Validate: should be at least 500 characters for a real risk factors section\n",
    "    if len(item_1a_text) < 500:\n",
    "        return None\n",
    "    \n",
    "    return item_1a_text\n",
    "\n",
    "print(\"Item 1A extraction function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the text column (varies by dataset structure)\n",
    "text_col = None\n",
    "for col in ['text', 'section_text', 'filing_text', 'content']:\n",
    "    if col in df_10k.columns:\n",
    "        text_col = col\n",
    "        break\n",
    "\n",
    "if text_col:\n",
    "    print(f\"Using text column: '{text_col}'\")\n",
    "else:\n",
    "    print(\"Available columns:\", df_10k.columns.tolist())\n",
    "    print(\"\\nPlease identify the correct text column\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Item 1A from all filings\n",
    "if text_col:\n",
    "    print(\"Extracting Item 1A sections...\")\n",
    "    tqdm.pandas(desc=\"Extracting\")\n",
    "    df_10k['item_1a'] = df_10k[text_col].progress_apply(extract_item_1a)\n",
    "    \n",
    "    # Check extraction success rate\n",
    "    extracted_count = df_10k['item_1a'].notna().sum()\n",
    "    total_count = len(df_10k)\n",
    "    print(f\"\\nExtraction results:\")\n",
    "    print(f\"  Successfully extracted: {extracted_count:,} ({extracted_count/total_count*100:.1f}%)\")\n",
    "    print(f\"  Failed/Not found: {total_count - extracted_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to only filings with Item 1A extracted\n",
    "df_risk = df_10k[df_10k['item_1a'].notna()].copy()\n",
    "print(f\"Final dataset size: {len(df_risk):,} filings with Risk Factors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sample Filings for Manual Review\n",
    "\n",
    "Let's examine 3-5 sample filings to understand the structure of Item 1A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample diverse filings (different years, sizes)\n",
    "if len(df_risk) > 0:\n",
    "    # Get samples from different years\n",
    "    samples = df_risk.groupby('filing_year').apply(\n",
    "        lambda x: x.sample(min(1, len(x))),\n",
    "        include_groups=False\n",
    "    ).head(5)\n",
    "    \n",
    "    print(\"Sample filings for manual review:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for idx, (_, row) in enumerate(samples.iterrows()):\n",
    "        print(f\"\\n--- Sample {idx + 1} (Year: {row.get('filing_year', 'N/A')}) ---\")\n",
    "        \n",
    "        # Print metadata if available\n",
    "        for col in ['cik', 'company_name', 'ticker', 'sic_code', 'filename']:\n",
    "            if col in row.index and pd.notna(row[col]):\n",
    "                print(f\"{col}: {row[col]}\")\n",
    "        \n",
    "        # Print first 2000 characters of Item 1A\n",
    "        item_1a_text = row['item_1a']\n",
    "        print(f\"\\nItem 1A length: {len(item_1a_text):,} characters\")\n",
    "        print(f\"\\nFirst 2000 characters:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(item_1a_text[:2000])\n",
    "        print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate text statistics\n",
    "df_risk['item_1a_length'] = df_risk['item_1a'].str.len()\n",
    "df_risk['item_1a_word_count'] = df_risk['item_1a'].str.split().str.len()\n",
    "\n",
    "print(\"Item 1A Text Statistics:\")\n",
    "print(f\"\\nCharacter count:\")\n",
    "print(df_risk['item_1a_length'].describe())\n",
    "print(f\"\\nWord count:\")\n",
    "print(df_risk['item_1a_word_count'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filings per year\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Filings per year\n",
    "filings_by_year = df_risk.groupby('filing_year').size()\n",
    "axes[0, 0].bar(filings_by_year.index, filings_by_year.values, color='steelblue')\n",
    "axes[0, 0].set_xlabel('Year')\n",
    "axes[0, 0].set_ylabel('Number of Filings')\n",
    "axes[0, 0].set_title('10-K Filings with Item 1A by Year')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 2: Distribution of document lengths\n",
    "axes[0, 1].hist(df_risk['item_1a_word_count'], bins=50, color='steelblue', edgecolor='white')\n",
    "axes[0, 1].set_xlabel('Word Count')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Distribution of Item 1A Word Counts')\n",
    "axes[0, 1].axvline(df_risk['item_1a_word_count'].median(), color='red', linestyle='--', label=f\"Median: {df_risk['item_1a_word_count'].median():,.0f}\")\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Plot 3: Average word count by year\n",
    "avg_words_by_year = df_risk.groupby('filing_year')['item_1a_word_count'].mean()\n",
    "axes[1, 0].plot(avg_words_by_year.index, avg_words_by_year.values, marker='o', color='steelblue', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Year')\n",
    "axes[1, 0].set_ylabel('Average Word Count')\n",
    "axes[1, 0].set_title('Average Item 1A Length Over Time')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 4: Word count by year (boxplot)\n",
    "df_risk.boxplot(column='item_1a_word_count', by='filing_year', ax=axes[1, 1])\n",
    "axes[1, 1].set_xlabel('Year')\n",
    "axes[1, 1].set_ylabel('Word Count')\n",
    "axes[1, 1].set_title('Item 1A Word Count Distribution by Year')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "plt.suptitle('')  # Remove automatic title\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/eda_overview.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFigure saved to outputs/eda_overview.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Industry analysis (if SIC codes available)\n",
    "sic_col = None\n",
    "for col in ['sic_code', 'sic', 'industry_code']:\n",
    "    if col in df_risk.columns:\n",
    "        sic_col = col\n",
    "        break\n",
    "\n",
    "if sic_col:\n",
    "    print(f\"\\nFilings by Industry (SIC Code - Top 20):\")\n",
    "    sic_counts = df_risk[sic_col].value_counts().head(20)\n",
    "    print(sic_counts)\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    sic_counts.plot(kind='bar', ax=ax, color='steelblue')\n",
    "    ax.set_xlabel('SIC Code')\n",
    "    ax.set_ylabel('Number of Filings')\n",
    "    ax.set_title('Top 20 Industries by Filing Count')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../outputs/industry_distribution.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"SIC code column not found in dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Processed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns to save\n",
    "# Adjust based on available columns\n",
    "cols_to_keep = ['filing_year', 'item_1a', 'item_1a_length', 'item_1a_word_count']\n",
    "\n",
    "# Add metadata columns if they exist\n",
    "for col in ['cik', 'company_name', 'ticker', 'sic_code', 'filename', 'accession_number']:\n",
    "    if col in df_risk.columns:\n",
    "        cols_to_keep.insert(0, col)\n",
    "\n",
    "# Keep only existing columns\n",
    "cols_to_keep = [c for c in cols_to_keep if c in df_risk.columns]\n",
    "\n",
    "df_final = df_risk[cols_to_keep].copy()\n",
    "\n",
    "print(f\"Final dataset shape: {df_final.shape}\")\n",
    "print(f\"Columns: {df_final.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to parquet (efficient for large text data)\n",
    "output_path = '../data/processed/risk_factors_2006_2020.parquet'\n",
    "df_final.to_parquet(output_path, index=False)\n",
    "print(f\"Dataset saved to: {output_path}\")\n",
    "print(f\"File size: {pd.io.common.file_size(output_path) / 1024 / 1024:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also save a smaller CSV sample for quick inspection\n",
    "sample_path = '../data/processed/risk_factors_sample_1000.csv'\n",
    "df_final.sample(min(1000, len(df_final))).to_csv(sample_path, index=False)\n",
    "print(f\"Sample saved to: {sample_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1 Summary\n",
    "\n",
    "### Dataset Overview\n",
    "- **Total 10-K filings processed**: [to be filled after running]\n",
    "- **Filings with Item 1A extracted**: [to be filled]\n",
    "- **Date range**: 2006-2020\n",
    "\n",
    "### Key Observations\n",
    "1. Item 1A sections have grown longer over time (regulatory expansion)\n",
    "2. Median word count: ~X,XXX words\n",
    "3. Top industries: [to be filled]\n",
    "\n",
    "### Next Steps (Phase 2)\n",
    "- Build text preprocessing pipeline\n",
    "- Handle HTML artifacts and legal boilerplate\n",
    "- Implement sentence segmentation for classification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
